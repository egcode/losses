{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T13:00:45.951502",
     "start_time": "2017-04-23T13:00:44.296636"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import math\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(tf.__version__)\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "NUM_PARALLEL_CALLS = 2 # number of cpu cores\n",
    "BATCH_SIZE = 7\n",
    "REPEAT_DATASET = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T13:00:44.295728",
     "start_time": "2017-04-23T13:00:44.293871"
    },
    "collapsed": true
   },
   "source": [
    "## Dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_reader\n",
    "train_images, train_labels = mnist_reader.load_mnist('datasets/mnist', kind='train')\n",
    "test_images, test_labels = mnist_reader.load_mnist('datasets/mnist', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n-------------------------------\")\n",
    "print(\"train_images shape: \" + str(train_images.shape))\n",
    "print(\"train_labels shape: \" + str(train_labels.shape))\n",
    "print(\"valid_images shape: \" + str(test_images.shape))\n",
    "print(\"valid_labels shape: \" + str(test_labels.shape))\n",
    "print(\"-------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training datasets\n",
    "dx_train = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "\n",
    "# apply a one-hot transformation to each label for use in the neural network\n",
    "dy_train = tf.data.Dataset.from_tensor_slices(train_labels)\n",
    "dy_train = dy_train.map(lambda z: tf.one_hot(z, 10), num_parallel_calls=NUM_PARALLEL_CALLS)\n",
    "dy_train = dy_train.repeat()\n",
    "\n",
    "# zip the x and y training data together and shuffle, batch etc.\n",
    "train_dataset = tf.data.Dataset.zip((dx_train, dy_train)).shuffle(500)\n",
    "train_dataset = train_dataset.repeat(REPEAT_DATASET)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "# do the same for validation set\n",
    "dx_valid = tf.data.Dataset.from_tensor_slices(test_images)\n",
    "dy_valid = tf.data.Dataset.from_tensor_slices(test_labels)\n",
    "dy_valid = dy_valid.map(lambda z: tf.one_hot(z, 10), num_parallel_calls=NUM_PARALLEL_CALLS)\n",
    "dy_valid = dy_valid.repeat()\n",
    "\n",
    "test_dataset = tf.data.Dataset.zip((dx_valid, dy_valid)).shuffle(500)\n",
    "test_dataset = test_dataset.repeat(REPEAT_DATASET)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create general iterator\n",
    "iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                            train_dataset.output_shapes)\n",
    "next_batch = iterator.get_next()\n",
    "\n",
    "# make datasets that we can initialize separately, but using the same structure via the common iterator\n",
    "training_init_op = iterator.make_initializer(train_dataset)\n",
    "test_init_op = iterator.make_initializer(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "    input_images = tf.placeholder(tf.float32, shape=(None,28,28,1), name='input_images')\n",
    "    labels = tf.placeholder(tf.int64, shape=(None, NUM_CLASSES), name='labels')\n",
    "    \n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "\n",
    "def parametric_relu(_x):\n",
    "  alphas = tf.get_variable('alpha', _x.get_shape()[-1],\n",
    "                       initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float32)\n",
    "  pos = tf.nn.relu(_x)\n",
    "  neg = alphas * (_x - abs(_x)) * 0.5\n",
    "\n",
    "  return pos + neg\n",
    "\n",
    "\n",
    "def inference(input_images):\n",
    "    with slim.arg_scope([slim.conv2d], kernel_size=3, padding='SAME'):\n",
    "        with slim.arg_scope([slim.max_pool2d], kernel_size=2):\n",
    "            \n",
    "            x = slim.conv2d(input_images, num_outputs=32, scope='conv1_1')\n",
    "            x = slim.conv2d(x, num_outputs=32, scope='conv1_2')\n",
    "            x = slim.max_pool2d(x, scope='pool1')\n",
    "     \n",
    "            x = slim.conv2d(x, num_outputs=64, scope='conv2_1')\n",
    "            x = slim.conv2d(x, num_outputs=64, scope='conv2_2')\n",
    "            x = slim.max_pool2d(x, scope='pool2')\n",
    "            \n",
    "            x = slim.conv2d(x, num_outputs=128, scope='conv3_1')\n",
    "            x = slim.conv2d(x, num_outputs=128, scope='conv3_2')\n",
    "            x = slim.max_pool2d(x, scope='pool3')\n",
    "            \n",
    "            x = slim.flatten(x, scope='flatten')\n",
    "            \n",
    "            features3d = slim.fully_connected(x, num_outputs=3, activation_fn=None, scope='fc0')\n",
    "            features2d = slim.fully_connected(features3d, num_outputs=2, activation_fn=None, scope='fc1')\n",
    "\n",
    "            x = parametric_relu(features3d)\n",
    "            x = slim.fully_connected(x, num_outputs=10, activation_fn=None, scope='fc2')\n",
    "    \n",
    "    return x, features2d, features3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arcface_loss(embedding, labels, out_num, w_init=None, s=64., m=0.5):\n",
    "    '''\n",
    "    :param embedding: the input embedding vectors\n",
    "    :param labels:  the input labels, the shape should be eg: (batch_size, 1)\n",
    "    :param s: scalar value default is 64\n",
    "    :param out_num: output class num\n",
    "    :param m: the margin value, default is 0.5\n",
    "    :return: the final cacualted output, this output is send into the tf.nn.softmax directly\n",
    "    '''\n",
    "    cos_m = math.cos(m)\n",
    "    sin_m = math.sin(m)\n",
    "    mm = sin_m * m  # issue 1\n",
    "    threshold = math.cos(math.pi - m)\n",
    "    with tf.variable_scope('arcface_loss'):\n",
    "        # inputs and weights norm\n",
    "        embedding_norm = tf.norm(embedding, axis=1, keep_dims=True)\n",
    "        embedding = tf.div(embedding, embedding_norm, name='norm_embedding')\n",
    "        weights = tf.get_variable(name='embedding_weights', shape=(embedding.get_shape().as_list()[-1], out_num),\n",
    "                                  initializer=w_init, dtype=tf.float32)\n",
    "        weights_norm = tf.norm(weights, axis=0, keep_dims=True)\n",
    "        weights = tf.div(weights, weights_norm, name='norm_weights')\n",
    "        # cos(theta+m)\n",
    "        cos_t = tf.matmul(embedding, weights, name='cos_t')\n",
    "        cos_t2 = tf.square(cos_t, name='cos_2')\n",
    "        sin_t2 = tf.subtract(1., cos_t2, name='sin_2')\n",
    "        sin_t = tf.sqrt(sin_t2, name='sin_t')\n",
    "        cos_mt = s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')\n",
    "\n",
    "        # this condition controls the theta+m should in range [0, pi]\n",
    "        #      0<=theta+m<=pi\n",
    "        #     -m<=theta<=pi-m\n",
    "        cond_v = cos_t - threshold\n",
    "        cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)\n",
    "\n",
    "        keep_val = s*(cos_t - mm)\n",
    "        cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n",
    "\n",
    "        mask = tf.one_hot(labels, depth=out_num, name='one_hot_mask')\n",
    "        # mask = tf.squeeze(mask, 1)\n",
    "        inv_mask = tf.subtract(1., mask, name='inverse_mask')\n",
    "\n",
    "        s_cos_t = tf.multiply(s, cos_t, name='scalar_cos_t')\n",
    "\n",
    "        output = tf.add(tf.multiply(s_cos_t, inv_mask), tf.multiply(cos_mt_temp, mask), name='arcface_loss_output')\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(logits, features, labels, ratio=0.5):\n",
    "    with tf.name_scope('loss'):\n",
    "#         with tf.name_scope('center_loss'):\n",
    "#             label_non_one_hot = tf.argmax(labels, 1)\n",
    "#             center_loss, centers, centers_update_op = get_center_loss(features, label_non_one_hot, CENTER_LOSS_ALPHA, NUM_CLASSES)        \n",
    "#         with tf.name_scope('softmax_loss'):\n",
    "#             softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits))\n",
    "\n",
    "#         with tf.name_scope('total_loss'):\n",
    "#             softmax_loss_index = tf.cast(softmax_loss, tf.float32)\n",
    "#             total_loss = softmax_loss_index + ratio * center_loss            \n",
    "\n",
    "            \n",
    "            \n",
    "        with tf.name_scope('total_loss'):\n",
    "            label_non_one_hot = tf.argmax(labels, 1)\n",
    "            w_init_method = tf.contrib.layers.xavier_initializer(uniform=False)\n",
    "            \n",
    "            logit = arcface_loss(embedding=logits, labels=label_non_one_hot, w_init=w_init_method, out_num=NUM_CLASSES)\n",
    "            inference_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=label_non_one_hot))\n",
    "            \n",
    "            total_loss = inference_loss \n",
    "\n",
    "    with tf.name_scope('loss/'):\n",
    "#         tf.summary.scalar('SoftmaxLoss', softmax_loss)\n",
    "#         tf.summary.scalar('CenterLoss', center_loss)\n",
    "#         tf.summary.scalar('TotalLoss', total_loss)\n",
    "        tf.summary.scalar('ArcfaceLoss', total_loss)\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def accuracy(labels):\n",
    "    with tf.name_scope('acc'):\n",
    "        prediction = tf.argmax(logits, 1)        \n",
    "        equality = tf.equal(prediction, tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, features2d, features3d = inference(input_images)\n",
    "loss = loss(logits, features3d, labels)\n",
    "accuracy = accuracy(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = tf.argmax(logits, 1)\n",
    "init_op = tf.global_variables_initializer()\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_data = np.mean(train_images, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python import debug as tf_debug\n",
    "\n",
    "# run the training\n",
    "epochs = 8000\n",
    "sess = tf.Session()\n",
    "# sess = tf_debug.TensorBoardDebugWrapperSession(sess, \"Eugenes-MBP:7000\")\n",
    "# Start Tensorflow from this file 'tensorboard --logdir=/tmp/mnist_log --debugger_port 7000'\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(init_op)\n",
    "sess.run(training_init_op)\n",
    "\n",
    "writer = tf.summary.FileWriter('/tmp/mnist_log', sess.graph)\n",
    "# Start Tensorflow from this file 'tensorboard --logdir=/tmp/mnist_log'\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    img_batch, label_batch = sess.run(next_batch)\n",
    "    l, _, acc, summary_str = sess.run([loss, optimizer, accuracy, summary_op],\n",
    "                                         feed_dict={input_images: img_batch-mean_data,labels: label_batch})\n",
    "\n",
    "    writer.add_summary(summary_str, global_step=epochs)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(\"Epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%\".format(i, l, acc * 100)) \n",
    "\n",
    "\n",
    "\n",
    "# now setup the validation run\n",
    "test_iters = 100\n",
    "# re-initialize the iterator, but this time with validation data\n",
    "sess.run(test_init_op)\n",
    "avg_acc = 0\n",
    "for i in range(test_iters):\n",
    "    img_batch, label_batch = sess.run(next_batch)\n",
    "    acc = sess.run([accuracy], feed_dict={input_images: img_batch-mean_data,labels: label_batch})\n",
    "    avg_acc += acc[0]\n",
    "print(\"Average validation set accuracy over {} iterations is {:.2f}%\".format(test_iters, (avg_acc / test_iters) * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = sess.run(features2d, feed_dict={input_images:train_images[:20000]-mean_data})\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = train_labels[:20000]\n",
    "\n",
    "f = plt.figure(figsize=(16,9))\n",
    "c = ['#ff0000', '#ffff00', '#00ff00', '#00ffff', '#0000ff', \n",
    "     '#ff00ff', '#990000', '#999900', '#009900', '#009999']\n",
    "for i in range(10):\n",
    "    plt.plot(feat[labels==i,0].flatten(), feat[labels==i,1].flatten(), '.', c=c[i])\n",
    "plt.legend(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T13:02:30.484009",
     "start_time": "2017-04-23T13:02:29.879168"
    }
   },
   "outputs": [],
   "source": [
    "feat = sess.run(features2d, feed_dict={input_images:test_images[:20000]-mean_data})\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = test_labels[:20000]\n",
    "\n",
    "f = plt.figure(figsize=(16,9))\n",
    "c = ['#ff0000', '#ffff00', '#00ff00', '#00ffff', '#0000ff', \n",
    "     '#ff00ff', '#990000', '#999900', '#009900', '#009999']\n",
    "for i in range(10):\n",
    "    plt.plot(feat[labels==i,0].flatten(), feat[labels==i,1].flatten(), '.', c=c[i])\n",
    "plt.legend(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize train_data 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "feat = sess.run(features3d, feed_dict={input_images:train_images[:10000]-mean_data})\n",
    "labels = train_labels[:10000]\n",
    "\n",
    "fig = plt.figure(figsize=(16,9))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "for i in range(10):\n",
    "    # Data for three-dimensional scattered points\n",
    "    xdata = feat[labels==i,2].flatten()\n",
    "    ydata = feat[labels==i,0].flatten()\n",
    "    zdata = feat[labels==i,1].flatten()\n",
    "    ax.scatter3D(xdata, ydata, zdata);\n",
    "ax.legend(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'],loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize test_data 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "feat = sess.run(features3d, feed_dict={input_images:test_images[:10000]-mean_data})\n",
    "labels = test_labels[:10000]\n",
    "\n",
    "fig = plt.figure(figsize=(16,9))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "for i in range(10):\n",
    "    # Data for three-dimensional scattered points\n",
    "    xdata = feat[labels==i,2].flatten()\n",
    "    ydata = feat[labels==i,0].flatten()\n",
    "    zdata = feat[labels==i,1].flatten()\n",
    "    ax.scatter3D(xdata, ydata, zdata);\n",
    "ax.legend(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'],loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
